{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import unicode_literals\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class biLM(nn.Module):\n",
    "    '''\n",
    "    initialize with\n",
    "    embedding: pre-trained embedding layer\n",
    "    hidden_size: size of hidden_states of biLM\n",
    "    n_layers: number of layers\n",
    "    dropout: dropout\n",
    "    '''\n",
    "    def __init__(self, hidden_size, embedding, n_layers=1, dropout=0):\n",
    "        super(biLM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.embedding = embedding\n",
    "        USE_CUDA = torch.cuda.is_available()\n",
    "        self.device = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n",
    "        self.forwardLSTM = nn.LSTM(hidden_size, \n",
    "                                         hidden_size, \n",
    "                                         n_layers, \n",
    "                                         dropout=(0 if n_layers == 1 else dropout))\n",
    "        self.backwardLSTM = nn.LSTM(hidden_size, \n",
    "                                         hidden_size, \n",
    "                                         n_layers, \n",
    "                                         dropout=(0 if n_layers == 1 else dropout))\n",
    "        \n",
    "    def forward(self, input_seq, input_lengths, initial_states=None):\n",
    "        '''\n",
    "        input_seq: size=(MAX_LEN, batch_size)\n",
    "        input_lengths: contains length of each sentence\n",
    "        initial_states: tuple of initial hidden_state of LSTM, initial cell state of LSTM\n",
    "        '''\n",
    "        embedded = self.embedding(input_seq)\n",
    "        MAX_LEN = embedded.size()[0]\n",
    "        batch_size = embedded.size()[1]\n",
    "        # embedded: size=(MAX_LEN, batch_size, hidden_size)\n",
    "        outputs = torch.zeros(MAX_LEN, batch_size, 2, self.hidden_size, device=self.device)\n",
    "        hidden_states = torch.zeros(self.n_layers * 2, MAX_LEN, batch_size, self.hidden_size, device=self.device)\n",
    "        \n",
    "        if not initial_states:\n",
    "            initial_states = (torch.zeros(self.n_layers, 1, self.hidden_size, device=self.device), torch.zeros(self.n_layers, 1, self.hidden_size, device=self.device))\n",
    "        \n",
    "        for batch_n in range(batch_size):\n",
    "            sentence = embedded[:,batch_n, :]\n",
    "            length = input_lengths[batch_n]\n",
    "            \n",
    "            \n",
    "            hidden_forward_state, cell_forward_state = initial_states\n",
    "            hidden_backward_state, cell_backward_state = initial_states\n",
    "                \n",
    "            for t in range(length):\n",
    "                output, (hidden_forward_state, cell_forward_state) = self.forwardLSTM(sentence[t].view(1, 1, -1), (hidden_forward_state, cell_forward_state))\n",
    "                outputs[t, batch_n, 0, :] = output[0, 0, :]\n",
    "                hidden_states[:self.n_layers, t, batch_n, :] = hidden_forward_state[:, 0, :]\n",
    "                \n",
    "            for t in range(length):\n",
    "                output, (hidden_backward_state, cell_backward_state) = self.backwardLSTM(sentence[length - t - 1].view(1, 1, -1), (hidden_backward_state, cell_backward_state))\n",
    "                outputs[length - t - 1, batch_n, 1, :] = output[0, 0, :]\n",
    "                hidden_states[self.n_layers:, length - t - 1, batch_n, :] = hidden_backward_state[:, 0, :]\n",
    "                \n",
    "        return outputs, hidden_states, embedded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class ELMo(nn.Module):\n",
    "    '''\n",
    "    initialize with\n",
    "    \n",
    "    '''\n",
    "    def __init__(self, hidden_size, embedding, n_layers=1, dropout=0, l2_coef=None, do_layer_norm=False):\n",
    "        super(ELMo, self).__init__()\n",
    "        USE_CUDA = torch.cuda.is_available()\n",
    "        self.device = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n",
    "        self.hidden_size = hidden_size\n",
    "        self.l2_coef = l2_coef\n",
    "        self.do_layer_norm = do_layer_norm\n",
    "        self.n_layers = n_layers\n",
    "        self.biLM = biLM(hidden_size, embedding, n_layers, dropout)\n",
    "        self.W = torch.zeros(2*n_layers+1, requires_grad=True, device=self.device)\n",
    "        self.W = F.softmax(self.W + 1/(2*n_layers + 1))\n",
    "        self.gamma = torch.ones(1, requires_grad=True, device=self.device)\n",
    "        \n",
    "    def do_norm(layer, mask):\n",
    "        masked_layer = layer * mask\n",
    "        N = torch.sum(mask) * self.hidden_size\n",
    "        mean = torch.sum(masked_layer)/N\n",
    "        variance = torch.sum(((masked_layer - mean) * mask) ** 2) / N\n",
    "        \n",
    "        return F.batch_norm(layer, mean, variance)\n",
    "    \n",
    "    def forward(self, input_seq, input_lengths, mask, initial_states=None):\n",
    "        bilm_outputs, hidden_states, embedded = self.biLM(input_seq, input_lengths, initial_states)\n",
    "        concat_hidden_with_embedding = torch.cat((embedded.unsqueeze(0), hidden_states), dim=0)\n",
    "        ELMo_embedding = torch.zeros(*embedded.size(), device=self.device)\n",
    "        \n",
    "        for i in range(2*self.n_layers + 1):\n",
    "            w = self.W[i]\n",
    "            layer = concat_hidden_with_embedding[i]\n",
    "            if self.do_layer_norm:\n",
    "                layer = self.do_norm(layer, mask)\n",
    "            ELMo_embedding = ELMo_embedding + w * layer\n",
    "        ELMo_embedding *= self.gamma.item()\n",
    "        return ELMo_embedding, bilm_outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
