{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import unicode_literals\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if USE_CUDA else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from classes import biLM, ELMo\n",
    "from data_utils import Vocab, batcher, data_loader, make_confusion_matrix, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = 'sentiment/data.txt'\n",
    "embedding_file = 'glove.6B.50d.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words, sentiment = ['later', 'i', 'found', 'myself', 'lost', 'in', 'the', 'power', 'of', 'the', 'film'] 1\n",
      "X, Y = tensor([ 168,   41,  238, 3261,  402,    6,    0,  268,    3,    0,  319],\n",
      "       device='cuda:0') tensor(1, device='cuda:0')\n",
      "words, sentiment = ['all', 'in', 'all', 'its', 'an', 'insult', 'to', \"one's\", 'intelligence', 'and', 'a', 'huge', 'waste', 'of', 'money'] 0\n",
      "X, Y = tensor([    64,      6,     64,     47,     29,  12965,      4, 400001,   1226,\n",
      "             5,      7,   1324,   3631,      3,    308], device='cuda:0') tensor(0, device='cuda:0')\n",
      "words, sentiment = ['well', 'im', 'satisfied'] 1\n",
      "X, Y = tensor([  143, 14663,  5456], device='cuda:0') tensor(1, device='cuda:0')\n",
      "words, sentiment = ['you', 'can', 'not', 'answer', 'calls', 'with', 'the', 'unit', 'never', 'worked', 'once'] 0\n",
      "X, Y = tensor([  81,   86,   36, 2168,  971,   17,    0, 1207,  332,  762,  442],\n",
      "       device='cuda:0') tensor(0, device='cuda:0')\n",
      "words, sentiment = ['waitress', 'was', 'sweet', 'and', 'funny'] 1\n",
      "X, Y = tensor([19644,    15,  3714,     5,  5466], device='cuda:0') tensor(1, device='cuda:0')\n",
      "words, sentiment = ['then', 'as', 'if', 'i', \"hadn't\", 'wasted', 'enough', 'of', 'my', 'life', 'there', 'they', 'poured', 'salt', 'in', 'the', 'wound', 'by', 'drawing', 'out', 'the', 'time', 'it', 'took', 'to', 'bring', 'the', 'check'] 0\n",
      "X, Y = tensor([   127,     19,     83,     41, 400001,  10939,    575,      3,    192,\n",
      "           214,     63,     39,   8096,   2982,      6,      0,   5868,     21,\n",
      "          3687,     66,      0,     79,     20,    247,      4,    938,      0,\n",
      "          2375], device='cuda:0') tensor(0, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "data = data_loader(data_file, vocab, print_every=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Task(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, embedding, use_ELMo=True, n_layers=1, bidirectional=True, dropout=0.5):\n",
    "        super(Task, self).__init__()\n",
    "        USE_CUDA = torch.cuda.is_available()\n",
    "        self.device = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n",
    "        self.embedding = embedding\n",
    "        self.use_ELMo = use_ELMo\n",
    "        self.drop1 = nn.Dropout(p=dropout)\n",
    "        self.output_dim = output_dim\n",
    "        if bidirectional:\n",
    "            self.num_dir = 2\n",
    "        else:\n",
    "            self.num_dir = 1\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, n_layers, dropout=(0 if n_layers==1 else dropout), bidirectional=bidirectional)\n",
    "        self.drop2 = nn.Dropout(p=dropout)\n",
    "        self.dense = nn.Linear(hidden_dim*self.num_dir, output_dim)\n",
    "        self.soft = nn.Softmax(dim=-1)\n",
    "        \n",
    "    def forward(self, input_seq, input_lengths, mask):\n",
    "        input_seq = input_seq.t()\n",
    "        mask = mask.t()\n",
    "        if self.use_ELMo:\n",
    "#             flg = True\n",
    "#             if flg:\n",
    "#                 flg = False\n",
    "#                 print(input_seq.size(), input_lengths.size(), mask.size())\n",
    "            embedded, _ = self.embedding(input_seq, input_lengths, mask)\n",
    "        else:\n",
    "            embedded = self.embedding(input_seq)\n",
    "        outputs = torch.zeros(embedded.size()[1], self.output_dim, device=self.device)\n",
    "        for batch in range(embedded.size()[1]):\n",
    "            dropped1 = self.drop1(embedded[:,batch,:])\n",
    "            out, _ = self.lstm(torch.unsqueeze(dropped1[:input_lengths[batch], :], 1))\n",
    "            dropped2 = self.drop2(out[-1,0,:])\n",
    "            densed = self.dense(dropped2)\n",
    "            softed = self.soft(densed)\n",
    "            outputs[batch, :] = softed\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 15\n",
    "batch_size = 256\n",
    "test_split_ratio = 0.25\n",
    "input_dim = 50\n",
    "hidden_dim = 25\n",
    "output_dim = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = batcher(data, MAX_LEN, batch_size, test_ratio=test_split_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Task(input_dim, hidden_dim, output_dim, vocab.embedding, use_ELMo=False, dropout=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.002\n",
    "clip = 50.0\n",
    "n_epochs = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_optimizer = optim.Adam(model.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test):\n",
    "    with torch.no_grad():\n",
    "        model = model.to(device)\n",
    "        model.eval()\n",
    "        \n",
    "        test_true = []\n",
    "        test_pred = []\n",
    "\n",
    "        for batch in test:\n",
    "            test_X, test_Y, mask, lengths = batch\n",
    "            test_X = test_X.to(device)\n",
    "            test_Y = test_Y.to(device)\n",
    "            mask = mask.to(device)\n",
    "            lengths = torch.tensor(lengths, device=device)\n",
    "\n",
    "            pred_prob = model(test_X.t(), lengths, mask.t())\n",
    "            pred_cls = torch.argmax(pred_prob, -1)\n",
    "            \n",
    "            test_pred += pred_cls.tolist()\n",
    "            test_true += test_Y.tolist()\n",
    "            \n",
    "        test_mat = make_confusion_matrix(test_true, test_pred)\n",
    "        \n",
    "        total_pred = np.sum(test_mat)\n",
    "        true_pred = sum([test_mat[i][i] for i in range(test_mat.shape[0])])\n",
    "        \n",
    "        print(\"test acc =\", true_pred / total_pred)\n",
    "        \n",
    "    return test_mat, test_pred, test_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, model_optimizer, train, test, n_epochs, clip, device):\n",
    "    from random import shuffle\n",
    "    \n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "  # dim = 0 [30, xxx] -> [10, ...], [10, ...], [10, ...] on 3 GPUs\n",
    "        model = nn.DataParallel(model)\n",
    "\n",
    "    model.to(device)\n",
    "    \n",
    "    train_true = []\n",
    "    train_pred = []\n",
    "    train_loss = []\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        curr_true = []\n",
    "        curr_pred = []\n",
    "        model.train()\n",
    "        \n",
    "        for batch in train:\n",
    "            \n",
    "            model_optimizer.zero_grad()\n",
    "            \n",
    "            train_X, train_Y, mask, lengths = batch\n",
    "            train_X = train_X.to(device)\n",
    "            train_Y = train_Y.to(device)\n",
    "            mask = mask.to(device)\n",
    "            lengths = torch.tensor(lengths, device=device)\n",
    "            \n",
    "            pred_prob = model(train_X.t(), lengths, mask.t())\n",
    "            \n",
    "            loss_func = nn.CrossEntropyLoss()\n",
    "            loss = loss_func(pred_prob, train_Y)\n",
    "            loss = loss.to(device)\n",
    "            train_loss.append(loss.item() * lengths.size()[0])\n",
    "            \n",
    "            loss.backward(retain_graph=True)\n",
    "            \n",
    "            _ = nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "            model_optimizer.step()\n",
    "            \n",
    "            pred_cls = torch.argmax(pred_prob, -1)\n",
    "            \n",
    "            curr_pred += pred_cls.tolist()\n",
    "            curr_true += train_Y.tolist()\n",
    "        \n",
    "        train_true.append(curr_true)\n",
    "        train_pred.append(curr_true)\n",
    "        train_mat = make_confusion_matrix(curr_true, curr_pred)\n",
    "        \n",
    "        total_pred = np.sum(train_mat)\n",
    "        true_pred = sum([train_mat[i][i] for i in range(train_mat.shape[0])])\n",
    "        \n",
    "        cur_loss = sum(train_loss[-len(train):]) / total_pred\n",
    "        \n",
    "        print(\"epoch =\",epoch, \"loss = \", cur_loss.item(), \"train acc =\", true_pred / total_pred)\n",
    "        \n",
    "        evaluate(model, test[:4])\n",
    "        shuffle(test)\n",
    "        shuffle(train)\n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's use 4 GPUs!\n",
      "epoch = 0 loss =  0.6941838192939759 train acc = 0.5111111111111111\n",
      "test acc = 0.5106666666666667\n",
      "epoch = 1 loss =  0.690430541197459 train acc = 0.532\n",
      "test acc = 0.512\n",
      "epoch = 2 loss =  0.6904797972043355 train acc = 0.5262222222222223\n",
      "test acc = 0.5093333333333333\n",
      "epoch = 3 loss =  0.6874054523044162 train acc = 0.548\n",
      "test acc = 0.508\n",
      "epoch = 4 loss =  0.6874714207119412 train acc = 0.5471111111111111\n",
      "test acc = 0.5186666666666667\n",
      "epoch = 5 loss =  0.6819676832093133 train acc = 0.5737777777777778\n",
      "test acc = 0.516\n",
      "epoch = 6 loss =  0.6815665980445014 train acc = 0.5773333333333334\n",
      "test acc = 0.5266666666666666\n",
      "epoch = 7 loss =  0.6759660246107313 train acc = 0.5835555555555556\n",
      "test acc = 0.5373333333333333\n",
      "epoch = 8 loss =  0.6703267672326829 train acc = 0.5857777777777777\n",
      "test acc = 0.5693333333333334\n",
      "epoch = 9 loss =  0.6669372226397197 train acc = 0.6008888888888889\n",
      "test acc = 0.56\n",
      "epoch = 10 loss =  0.6603947127130296 train acc = 0.6186666666666667\n",
      "test acc = 0.564\n",
      "epoch = 11 loss =  0.6575804190105862 train acc = 0.628\n",
      "test acc = 0.5653333333333334\n",
      "epoch = 12 loss =  0.6499878720177544 train acc = 0.6266666666666667\n",
      "test acc = 0.5773333333333334\n",
      "epoch = 13 loss =  0.6533438425593906 train acc = 0.624\n",
      "test acc = 0.5786666666666667\n",
      "epoch = 14 loss =  0.6509441916147868 train acc = 0.6182222222222222\n",
      "test acc = 0.5813333333333334\n",
      "epoch = 15 loss =  0.6411461981667412 train acc = 0.6488888888888888\n",
      "test acc = 0.58\n",
      "epoch = 16 loss =  0.6389944188859727 train acc = 0.6337777777777778\n",
      "test acc = 0.5786666666666667\n",
      "epoch = 17 loss =  0.6365246705479092 train acc = 0.6475555555555556\n",
      "test acc = 0.5786666666666667\n",
      "epoch = 18 loss =  0.6351490677727594 train acc = 0.6528888888888889\n",
      "test acc = 0.592\n",
      "epoch = 19 loss =  0.6248359198040433 train acc = 0.6688888888888889\n",
      "test acc = 0.596\n",
      "epoch = 20 loss =  0.6240775462256537 train acc = 0.6742222222222222\n",
      "test acc = 0.6013333333333334\n",
      "epoch = 21 loss =  0.629414810233646 train acc = 0.6573333333333333\n",
      "test acc = 0.6133333333333333\n",
      "epoch = 22 loss =  0.6209898065990872 train acc = 0.6671111111111111\n",
      "test acc = 0.6146666666666667\n",
      "epoch = 23 loss =  0.6178547362751431 train acc = 0.6608888888888889\n",
      "test acc = 0.6\n",
      "epoch = 24 loss =  0.6166385649575128 train acc = 0.6733333333333333\n",
      "test acc = 0.6146666666666667\n",
      "epoch = 25 loss =  0.6086499337620206 train acc = 0.6822222222222222\n",
      "test acc = 0.616\n",
      "epoch = 26 loss =  0.610363176981608 train acc = 0.6773333333333333\n",
      "test acc = 0.6226666666666667\n",
      "epoch = 27 loss =  0.6030383507410685 train acc = 0.6884444444444444\n",
      "test acc = 0.636\n",
      "epoch = 28 loss =  0.5955522037082248 train acc = 0.7031111111111111\n",
      "test acc = 0.6506666666666666\n",
      "epoch = 29 loss =  0.5946746393309699 train acc = 0.6995555555555556\n",
      "test acc = 0.6386666666666667\n",
      "epoch = 30 loss =  0.5874197594854567 train acc = 0.7137777777777777\n",
      "test acc = 0.608\n",
      "epoch = 31 loss =  0.5986024230321249 train acc = 0.7004444444444444\n",
      "test acc = 0.6493333333333333\n"
     ]
    }
   ],
   "source": [
    "train_loss_without_ELMo = train_model(model, model_optimizer, train, test, n_epochs, clip, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test acc = 0.6493333333333333\n"
     ]
    }
   ],
   "source": [
    "test_mat_w, test_pred_w, test_true_w = evaluate(model, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_with_ELMo = Task(input_dim,\n",
    "                       hidden_dim,\n",
    "                       output_dim,\n",
    "                       ELMo(input_dim, vocab.embedding, n_layers=2, dropout=0.4),\n",
    "                       use_ELMo=True,\n",
    "                       dropout=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_opt = optim.Adam(model_with_ELMo.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's use 4 GPUs!\n",
      "epoch = 0 loss =  0.6932003099123637 train acc = 0.5017777777777778\n",
      "test acc = 0.46\n",
      "epoch = 1 loss =  0.6926024600134956 train acc = 0.5213333333333333\n",
      "test acc = 0.4613333333333333\n",
      "epoch = 2 loss =  0.6918900816175673 train acc = 0.5204444444444445\n",
      "test acc = 0.464\n",
      "epoch = 3 loss =  0.6901715452406142 train acc = 0.5453333333333333\n",
      "test acc = 0.5173333333333333\n",
      "epoch = 4 loss =  0.6839425075319078 train acc = 0.5782222222222222\n",
      "test acc = 0.5373333333333333\n",
      "epoch = 5 loss =  0.6759990796513028 train acc = 0.604\n",
      "test acc = 0.5506666666666666\n",
      "epoch = 6 loss =  0.6707493426005046 train acc = 0.6004444444444444\n",
      "test acc = 0.532\n",
      "epoch = 7 loss =  0.6687986068195767 train acc = 0.5937777777777777\n",
      "test acc = 0.584\n",
      "epoch = 8 loss =  0.6602253426445855 train acc = 0.6102222222222222\n",
      "test acc = 0.576\n",
      "epoch = 9 loss =  0.6604879176351759 train acc = 0.6035555555555555\n",
      "test acc = 0.5733333333333334\n",
      "epoch = 10 loss =  0.6479963329633077 train acc = 0.6342222222222222\n",
      "test acc = 0.5853333333333334\n",
      "epoch = 11 loss =  0.6388157402144538 train acc = 0.6497777777777778\n",
      "test acc = 0.6053333333333333\n",
      "epoch = 12 loss =  0.6359441143141853 train acc = 0.6493333333333333\n",
      "test acc = 0.5746666666666667\n",
      "epoch = 13 loss =  0.6341017503208585 train acc = 0.6471111111111111\n",
      "test acc = 0.608\n",
      "epoch = 14 loss =  0.6215728944672478 train acc = 0.6688888888888889\n",
      "test acc = 0.628\n",
      "epoch = 15 loss =  0.6161996726459927 train acc = 0.6728888888888889\n",
      "test acc = 0.628\n",
      "epoch = 16 loss =  0.6061560980478923 train acc = 0.6822222222222222\n",
      "test acc = 0.6293333333333333\n",
      "epoch = 17 loss =  0.6047138399018182 train acc = 0.6951111111111111\n",
      "test acc = 0.6453333333333333\n",
      "epoch = 18 loss =  0.6049458971553379 train acc = 0.6791111111111111\n",
      "test acc = 0.6506666666666666\n",
      "epoch = 19 loss =  0.6030808196597629 train acc = 0.6866666666666666\n",
      "test acc = 0.6493333333333333\n",
      "epoch = 20 loss =  0.5870713544421726 train acc = 0.7026666666666667\n",
      "test acc = 0.6453333333333333\n",
      "epoch = 21 loss =  0.5771075108846029 train acc = 0.7191111111111111\n",
      "test acc = 0.6533333333333333\n",
      "epoch = 22 loss =  0.5769520591629876 train acc = 0.7142222222222222\n",
      "test acc = 0.6666666666666666\n",
      "epoch = 23 loss =  0.5786923879517449 train acc = 0.7146666666666667\n",
      "test acc = 0.6653333333333333\n",
      "epoch = 24 loss =  0.5687964046266344 train acc = 0.7306666666666667\n",
      "test acc = 0.68\n",
      "epoch = 25 loss =  0.5601821061240302 train acc = 0.7337777777777778\n",
      "test acc = 0.6746666666666666\n",
      "epoch = 26 loss =  0.5581937005254958 train acc = 0.7417777777777778\n",
      "test acc = 0.6866666666666666\n",
      "epoch = 27 loss =  0.5532212487326728 train acc = 0.7471111111111111\n",
      "test acc = 0.6346666666666667\n",
      "epoch = 28 loss =  0.5783134706285264 train acc = 0.72\n",
      "test acc = 0.6906666666666667\n",
      "epoch = 29 loss =  0.5522631173663669 train acc = 0.7462222222222222\n",
      "test acc = 0.684\n",
      "epoch = 30 loss =  0.5425139815807343 train acc = 0.7591111111111111\n",
      "test acc = 0.6773333333333333\n",
      "epoch = 31 loss =  0.5373492284880744 train acc = 0.7653333333333333\n",
      "test acc = 0.7013333333333334\n"
     ]
    }
   ],
   "source": [
    "train_loss = train_model(model_with_ELMo, model_opt, train, test, n_epochs, clip, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test acc = 0.7013333333333334\n"
     ]
    }
   ],
   "source": [
    "test_mat, test_pred, test_true = evaluate(model_with_ELMo, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
